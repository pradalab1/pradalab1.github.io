<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="utf-8">
      <title>PRADA Lab - Research</title>
             <!--网页标题左侧显示-->
             <link rel="icon" href="prada_home/logo22.png" type="image/x-icon">
             <!--收藏夹显示图标-->
             <link rel="shortcut icon" href="prada_home/logo22.png" type="image/x-icon">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="description" content="Research Lab, Home, Velit Aliquet Sagittis University">
      <meta name="author" content="">
      <!-- Le styles -->
      <link href="css/bootstrap.min.css" rel="stylesheet">
      <link href="css/bootstrap-responsive.min.css" rel="stylesheet">
      <link href="css/theme.css" rel="stylesheet">
   </head>
   <body>
      <div class="container">
         <header class="jumbotron subhead" id="overview">
            <p class="lead"> King Abdullah University of Science and Technology </p>
            <h1>Provable Responsible AI and Data Analytics Lab</h1>
         </header>
         <div class="masthead">
            <div class="navbar">
               <div class="navbar-inner">
                  <div class="container">
                     <ul class="nav">
                        <li><a href="index.html">Home</a></li>
                        <li><a href="people.html">People</a></li>
                        <li><a href="alumni.html">Alumni</a></li>
                        <li class="active"><a href="research.html">Research</a></li>
                        <li><a href="publications.html">Publications</a></li>
                        <li><a href="opening.html">Opening Positions</a></li>
                        <!--<li><a href="contact.html">Contact</a></li> -->
                     </ul>
                  </div>
               </div>
            </div>
         </div>
         <div class="row-fluid">
            <div class="span3 bs-docs-sidebar" id="navparent">
               <ul class="nav nav-list bs-docs-sidenav" data-spy="affix" data-offset-top="200" data-offset-bottom="260">
                  <li><a href="#state"> Research Areas </a></li>
                  <li><a href="#data"> Data </a></li>
                  <li><a class="subhead" href="#dp"> Differential Privacy </a></li>
                  <li><a class="subhead" href="#unlearn"> Machine Unlearning  </a></li>
                  <li><a class="subhead" href="#robust"> Adversarial Robustness </a></li>
                  <li><a href="#concept"> Concept </a></li>
                  <li><a class="subhead" href="#erasing"> Concept Erasing </a></li>
                  <li><a class="subhead" href="#xai"> Explainable AI </a></li>
                  <li><a href="#knowledge">Knowledge</a></li>
                  <li><a class="subhead" href="#edit"> Knowledge Editing </a></li>
                  <li><a href="#knowledge">AI for Science</a></li>
                  <li><a class="subhead" href="#onn"> Optical Neural Networks </a></li>
                  <li><a class="subhead" href="#optic"> AI for Optics </a></li>
                  <li><a href="#funding"> Research Fundings </a></li>
               </ul>
            </div>
            <div class="span8 offset1">
               <section id="state">
                  <div class="page-header">
                     <h3>Research Areas</h3>
                     <hr>
                     <p>We are  focusing on Trustworthy Machine Learning and AI for Science. In particular, we are interested in making Machine Learning and Generative AI have controllable and editable memorization, such as the memorization of data, knowledge, and concept, and its application to AI safety such as hallucination and copyright infringement: 
                        <o>
                           <li><b>Data:</b> data privacy, machine unlearning, adversarial robustness</li>
                           <li><b>Concept:</b> explainability, concept editing</li>
                           <li><b>Knowledge:</b> knowledge editing, alignment, culturalization, fake news detection</li>
                        </o>
                     </p>
                  </div>
               </section>
               <section id="data">
                  <div class="page-header">
                     <h3>Data</h3>
                  </div>
                  <div class="row-fluid">
                     <div class="span12">
                        <section id="dp">
                           <h4>Differential Privacy</h4>
                           <img src="research/dp.png" class="float-right" alt="Differential Privacy"/>
                           <p>
                              Differential Privacy is a method used to protect people’s private information when analyzing large sets of data. It ensures that anyone’s personal data can’t be identified, even when data is shared or used for research.   The main idea is to make sure that whether your data is included or not, the results of the data analysis won’t be noticeably different. This way, your privacy is protected because your individual information is hidden within the crowd.       </p>
                              
                           
                              
                              <p> <b>Research Topics:</b>  private stochastic optimzation,  private statistical estimation, DP-SGD, DP large model pre-training and fine tuning, privacy amplification, and privacy audit. We are also interested in privacy leackage attacks such as inference and reconstruction attacks. 
                           </p>
  
                        </section>
                        <hr>
                        <section id="unlearn">
                           <h4>Machine Unlearning</h4>
            
                           <img src="research/unlearn.jpg" class="float-right" alt="Machine Unlearning"/>
                           <p>
                              Imagine teaching a computer to recognize different animals using pictures. Over time, the computer gets better at identifying animals by learning from many images. However, if one of those images needs to be removed for privacy reasons, Machine Unlearning helps the computer forget what it learned from that specific image without having to start learning from scratch with all the other images.      </p>
                              
                              <p> The main goal of Machine Unlearning is to ensure that data can be removed effectively and that the influence of this data is erased from the model. This is crucial for scenarios where individuals request the deletion of their personal data, as required by laws like the General Data Protection Regulation (GDPR).
            
                                 <p> <b>Research Topics:</b> large model unlearning, unlearning for different types of data such as graphs, federated unlearning, unlearning evaluation, theory of machine unlearning.    
                           </p>
                           <p>
   
                           </p>
                        </section>
                        <hr>
                        <section id="robust">
                           <h4>Adversarial Robustness</h4>
                           <br/>
                           <img src="research/robust.png" class="float-right" alt="Adversarial Robustness"/>
                           <p>
                              Adversarial robustness is about making machine learning systems stronger and more reliable against tricks designed to fool them. These tricks, called adversarial attacks, involve making tiny, almost invisible changes to inputs like images or text to cause the system to make mistakes. For example, an attack might subtly alter a picture of a dog so a system misidentifies it as a cat, even though the change is imperceptible to humans. Adversarial robustness involves developing techniques and training methods to help these systems recognize and resist such manipulations. This is crucial for ensuring the security and dependability of AI in important areas like facial recognition, self-driving cars, and fraud detection, where mistakes can have serious consequences.
                           </p>
                           <p> <b>Research Topics:</b> certified robustness, theory of adversarial robustness, adversarial training. We are also interested in adversarial and backdoor attacks.        </p>
                        </section>
                     </div>
                  </div>
               </section>
               <hr>
               <section id="concept">
                  <div class="page-header">
                     <h3>Concept</h3>
                  </div>
                  <div class="row-fluid">
                     <div class="span12">
                        <section id="erasing">
                           <h4>Concept Erasing</h4>
                           <br/>
                           <img src="research/concept.png" class="float-right" alt="Concept Erasing"/>
                           <p>
                              Concept erasing is a technique used in machine learning to remove specific concpet including object, knowledge or biases from a trained model. Imagine teaching a computer to recognize various objects in pictures. Sometimes, the model might learn unwanted associations, like linking gender with certain professions. Concept erasing aims to “forget” these unwanted connections without affecting the model’s ability to perform its main tasks. This is done to ensure the AI behaves fairly and ethically, avoiding biases that could lead to discrimination or incorrect predictions. For example, if a model unfairly associates certain jobs with a specific gender, concept erasing would remove this bias, promoting more accurate and unbiased outcomes. This technique is important for creating fair, reliable, and ethical AI systems that make decisions based on relevant information rather than learned prejudices.
                           </p>
                           <p> <b>Research Topics:</b> fairness, locality, concept removal in diffusion models.       </p>
                        </section>
                        <hr>
                        <section id="xai">
                           <h4>Explainable AI</h4>
                           <br/>
                           <img src="research/xai.png" class="float-right" alt="Explainable AI"/>
                           <p>
                              Explainable AI (XAI) is a branch of artificial intelligence focused on making AI systems’ decisions transparent. It makes the black-box deep learning models to concepts that are understandable to humans. Imagine using a smart assistant that helps you with tasks like recommending books or suggesting routes for travel. Sometimes, its suggestions might seem puzzling. Explainable AI aims to provide clear reasons for these choices, making it easier to understand why the AI made certain recommendations. This is crucial in fields like healthcare or finance, where understanding the reasoning behind AI decisions can be critical. For example, if an AI recommends a particular medical treatment, XAI would enable doctors and patients to see the data and logic that led to this recommendation, ensuring transparency and trust. By making AI decisions more transparent, XAI helps users trust and verify the system’s fairness and accuracy, ultimately leading to more reliable and ethical AI applications.
                           </p>
                           <p>
                              <b>Research Topics:</b> concept models, faithfulness in XAI, XAI for Generative AI and large models.    
                           </p>
                        </section>
                        <hr>
                        <!--
                        <section id="FacilisisMagna">
                           <h4>Facilisis Magna</h4>
                           <br/>
                           <ol class="resources">
                              <li>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor 
                                 incididunt ut labore et dolore magna aliqua. p. 413-417, 2010.
                              </li>
                              <li>Sed ullamcorper morbi tincidunt ornare massa eget egestas. Suscipit tellus mauris 
                                 a diam. Facilisis magna etiam tempor orci eu lobortis elementum nibh tellus.  
                              </li>
                              <li>Tellus molestie nunc non blandit massa enim nec dui. Eget duis at tellus at urna 
                                 condimentum mattis pellentesque id. At urna condimentum mattis pellentesque id nibh 
                                 tortor. Placerat duis ultricies lacus sed turpis tincidunt id aliquet risus. 
                              </li>
                              <li>Laoreet non curabitur gravida arcu ac. Ultricies tristique nulla aliquet enim. Auctor 
                                 elit sed vulputate mi. Pellentesque diam volutpat commodo sed egestas egestas fringilla. 
                                 Eleifend donec pretium vulputate sapien nec sagittis. 38(5), p. 397-406, 2013.
                              </li>
                           </ol>
                        </section>
                        -->
                     </div>
                  </div>
               </section>
               <hr>
               <section id="knowledge">
                  <div class="page-header">
                     <h3>Knowledge</h3>
                  </div>
                  <div class="row-fluid">
                     <div class="span12">
                        <section id="edit">
                           <h4>Knowledge Editing</h4>
                           <br/>
                           <img src="research/edit.png" class="float-right" alt="Knowledge Editing"/>
                           <p>
                              Knowledge Editing involves updating or correcting specific information within these AI models without retraining them from scratch. LLMs, like those used in chatbots or virtual assistants, learn from vast amounts of text data to generate human-like responses. However, they can sometimes retain outdated or incorrect information. Knowledge Editing allows us to directly modify these specific pieces of information within the model, ensuring it remains accurate and reliable. For instance, if an LLM incorrectly states that a particular event occurred in 2020 instead of 2021, Knowledge Editing can correct this error without affecting the model’s overall functionality. This technique is essential because it enables the efficient and precise updating of LLMs, maintaining their relevance and accuracy without the time-consuming and resource-intensive process of full retraining. By employing Knowledge Editing, we can ensure that LLMs continue to provide correct and current information, enhancing their trustworthiness and utility in applications such as customer support, education, and content generation.
                           </p>
                           <p> <b>Research Topics:</b>  knowledge editing and unlearning in  Large Language Models and Multimodal Models.     </p>
                        </section>
                        <hr>
                      
            
                     </div>
                  </div>
               </section>

            </div>
         </div>
      </div>
      <footer id="footer">
         <div class="container-fluid">
            <div class="row-fluid">
               <div class="span5">
                  <h3>Contact Information</h3>
                  <p><b>Office Hours: </b>Sunday-Thursday (11.00am - 6.00pm)</p>
                  <p><b>Phone: </b>966-012-8080645</p>
                  <p><b>Cell: </b>966-012-8080645</p>
                  <a href="mailto:di.wang@kaust.edu.sa">Email</a>
               </div>
               <div class="span2">
                  <a href="https://pradalab1.github.io/index.html"><img src = "prada_home/logo.png" alt="research-lab-logo"/></a>
               </div>
               <div class="span5">
                  <h3>Address</h3>
                  <p>Al Khawarizmi Building 1, Room 4243<br>
                     King Abdullah University of Science and Technology<br>
                    Thuwal<br>
                     Saudi Arabia, 23955-6900
                  </p>
                  <a href="https://maps.app.goo.gl/5XzVDZDGg4mwpbUt9">Show Map</a>
               </div>
            </div>
         </div>
      </footer>

      <!-- Javascript files -->
      <script src="js/jquery-1.9.1.min.js"></script>
      <script src="js/bootstrap.min.js"></script>
      <script> $('#main-carousel').carousel(); </script>
   </body>
</html>